---
title: Introduction to Adversarial Machine Learning

event: MCSTalk
event_url: aut.ac.ir

location: Computer Science Department, AUT
address:
  street:
  city: Tehran
  region: Tehran
  postcode: ''
  country: Iran

summary: Can you hack deep learning models? Imagine your self driving car sees a "Stop" sign and thinks, yea that is definitely a "120km/h speed limit" sign.
abstract: With all the advances in machine learning and especially in deep learning, you may think that these models are robust and almost perfect in at least the easier tasks such as identifying animals. Unfortunately you're wrong. While these models have shown great achievements in many tasks, even better than humans, they're very vulnerable to a family of attacks called "Adversarial Attacks", an unsolved problem and an active field of research in machine learning, which we are going to talk about in this presentation.

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2023-06-05T13:00:00Z'
# date_end: '2023-06-05T15:00:00Z'
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: ''

authors: []
tags: []

# Is this a featured talk? (true/false)
featured: true

image:
  caption:
  focal_point:

links:
  - icon: 
    icon_pack: 
    name: 
    url: 
url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: 

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:
  - AUT
---
